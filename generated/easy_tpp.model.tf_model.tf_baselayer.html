<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>easy_tpp.model.tf_model.tf_baselayer &mdash; EasyTPP 0.0.2 documentation</title>
      <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="../_static/jquery.js"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/sphinx_highlight.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="easy_tpp.model.tf_model.tf_basemodel" href="easy_tpp.model.tf_model.tf_basemodel.html" />
    <link rel="prev" title="EasyTPP Models" href="../ref/models.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            EasyTPP
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">GETTING STARTED</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../get_started/introduction.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../get_started/install.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../get_started/quick_start.html">Qucick Start</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">USER GUIDE</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../user_guide/dataset.html">Dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="../user_guide/run_train_pipeline.html">Model Training</a></li>
<li class="toctree-l1"><a class="reference internal" href="../user_guide/run_eval.html">Model Prediction</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">DEVELOPER GUIDE</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../dev_guide/model_custom.html">Model Customization</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">ADVANCED TOPICS</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../advanced/thinning_algo.html">Thinning Algorithm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/tensorboard.html">Tensorboard</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/performance_valid.html">Performance Benchmarks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/implementation.html">Implementation Details</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API REFERENCE</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../ref/config.html"> Config</a></li>
<li class="toctree-l1"><a class="reference internal" href="../ref/preprocess.html"> Preprocess</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../ref/models.html"> Model</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="../ref/models.html#module-easy_tpp.model.tf_model">model.tf_model module</a><ul class="current">
<li class="toctree-l3 current"><a class="current reference internal" href="#">easy_tpp.model.tf_model.tf_baselayer</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#easy_tpp.model.tf_model.tf_baselayer.gelu"><code class="docutils literal notranslate"><span class="pre">gelu()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#easy_tpp.model.tf_model.tf_baselayer.append_tensor_alias"><code class="docutils literal notranslate"><span class="pre">append_tensor_alias()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#easy_tpp.model.tf_model.tf_baselayer.LayerNormalization"><code class="docutils literal notranslate"><span class="pre">LayerNormalization</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#easy_tpp.model.tf_model.tf_baselayer.MultiHeadAttention"><code class="docutils literal notranslate"><span class="pre">MultiHeadAttention</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#easy_tpp.model.tf_model.tf_baselayer.DenseDropoutLayernorm"><code class="docutils literal notranslate"><span class="pre">DenseDropoutLayernorm</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#easy_tpp.model.tf_model.tf_baselayer.EncoderLayer"><code class="docutils literal notranslate"><span class="pre">EncoderLayer</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#easy_tpp.model.tf_model.tf_baselayer.TimePositionalEncoding"><code class="docutils literal notranslate"><span class="pre">TimePositionalEncoding</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#easy_tpp.model.tf_model.tf_baselayer.TimeShiftedPositionalEncoding"><code class="docutils literal notranslate"><span class="pre">TimeShiftedPositionalEncoding</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#easy_tpp.model.tf_model.tf_baselayer.DNN"><code class="docutils literal notranslate"><span class="pre">DNN</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="easy_tpp.model.tf_model.tf_basemodel.html">easy_tpp.model.tf_model.tf_basemodel</a></li>
<li class="toctree-l3"><a class="reference internal" href="easy_tpp.model.tf_model.tf_nhp.html">easy_tpp.model.tf_model.tf_nhp</a></li>
<li class="toctree-l3"><a class="reference internal" href="easy_tpp.model.tf_model.tf_attnhp.html">easy_tpp.model.tf_model.tf_attnhp</a></li>
<li class="toctree-l3"><a class="reference internal" href="easy_tpp.model.tf_model.tf_thinning.html">easy_tpp.model.tf_model.tf_thinning</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../ref/models.html#module-easy_tpp.model.torch_model">model.torch_model module</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../ref/runner.html"> Runner</a></li>
<li class="toctree-l1"><a class="reference internal" href="../ref/hpo.html"> Hyper-parameter Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../ref/wrapper.html"> Tf and Torch Wrapper</a></li>
<li class="toctree-l1"><a class="reference internal" href="../ref/utils.html"> Utilities</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">EasyTPP</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../ref/models.html">EasyTPP Models</a></li>
      <li class="breadcrumb-item active">easy_tpp.model.tf_model.tf_baselayer</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/generated/easy_tpp.model.tf_model.tf_baselayer.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="module-easy_tpp.model.tf_model.tf_baselayer">
<span id="easy-tpp-model-tf-model-tf-baselayer"></span><h1>easy_tpp.model.tf_model.tf_baselayer<a class="headerlink" href="#module-easy_tpp.model.tf_model.tf_baselayer" title="Permalink to this heading"></a></h1>
<p class="rubric">Functions</p>
<table class="autosummary longtable docutils align-default">
<tbody>
<tr class="row-odd"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">activation_layer</span></code>(activation)</p></td>
<td><p></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#easy_tpp.model.tf_model.tf_baselayer.append_tensor_alias" title="easy_tpp.model.tf_model.tf_baselayer.append_tensor_alias"><code class="xref py py-obj docutils literal notranslate"><span class="pre">append_tensor_alias</span></code></a>(tensor, alias)</p></td>
<td><p>Append an alias to the list of aliases of the tensor.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#easy_tpp.model.tf_model.tf_baselayer.gelu" title="easy_tpp.model.tf_model.tf_baselayer.gelu"><code class="xref py py-obj docutils literal notranslate"><span class="pre">gelu</span></code></a>(x)</p></td>
<td><p>Gaussian Error Linear Unit.</p></td>
</tr>
<tr class="row-even"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">null_activation</span></code>(x)</p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">swish</span></code>(x)</p></td>
<td><p></p></td>
</tr>
</tbody>
</table>
<p class="rubric">Classes</p>
<table class="autosummary longtable docutils align-default">
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#easy_tpp.model.tf_model.tf_baselayer.DNN" title="easy_tpp.model.tf_model.tf_baselayer.DNN"><code class="xref py py-obj docutils literal notranslate"><span class="pre">DNN</span></code></a>(*args, **kwargs)</p></td>
<td><p>The Multi Layer Percetron Input shape   - nD tensor with shape: <code class="docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">...,</span> <span class="pre">input_dim)</span></code>. The most common situation would be a 2D input with shape <code class="docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">input_dim)</span></code>. Output shape   - nD tensor with shape: <code class="docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">...,</span> <span class="pre">hidden_size[-1])</span></code>. For instance, for a 2D input with shape <code class="docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">input_dim)</span></code>,   the output would have shape <code class="docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">hidden_size[-1])</span></code>. Arguments   - <strong>hidden_units</strong>:list of positive integer, the layer number and units in each layer. - <strong>activation</strong>: Activation function to use. - <strong>l2_reg</strong>: float between 0 and 1. L2 regularizer strength applied to the kernel weights matrix. - <strong>dropout_rate</strong>: float in [0,1). Fraction of the units to dropout. - <strong>use_bn</strong>: bool. Whether use BatchNormalization before activation or not. - <strong>output_activation</strong>: Activation function to use in the last layer.If <code class="docutils literal notranslate"><span class="pre">None</span></code>,   it will be same as <code class="docutils literal notranslate"><span class="pre">activation</span></code>. - <strong>seed</strong>: A Python integer to use as random seed.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#easy_tpp.model.tf_model.tf_baselayer.DenseDropoutLayernorm" title="easy_tpp.model.tf_model.tf_baselayer.DenseDropoutLayernorm"><code class="xref py py-obj docutils literal notranslate"><span class="pre">DenseDropoutLayernorm</span></code></a>(*args, **kwargs)</p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#easy_tpp.model.tf_model.tf_baselayer.EncoderLayer" title="easy_tpp.model.tf_model.tf_baselayer.EncoderLayer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">EncoderLayer</span></code></a>(*args, **kwargs)</p></td>
<td><p></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#easy_tpp.model.tf_model.tf_baselayer.LayerNormalization" title="easy_tpp.model.tf_model.tf_baselayer.LayerNormalization"><code class="xref py py-obj docutils literal notranslate"><span class="pre">LayerNormalization</span></code></a>(*args, **kwargs)</p></td>
<td><p>ref: <a class="reference external" href="https://github.com/shenweichen/DeepCTR/blob/master/deepctr/layers/normalization.py">https://github.com/shenweichen/DeepCTR/blob/master/deepctr/layers/normalization.py</a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#easy_tpp.model.tf_model.tf_baselayer.MultiHeadAttention" title="easy_tpp.model.tf_model.tf_baselayer.MultiHeadAttention"><code class="xref py py-obj docutils literal notranslate"><span class="pre">MultiHeadAttention</span></code></a>(*args, **kwargs)</p></td>
<td><p></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#easy_tpp.model.tf_model.tf_baselayer.TimePositionalEncoding" title="easy_tpp.model.tf_model.tf_baselayer.TimePositionalEncoding"><code class="xref py py-obj docutils literal notranslate"><span class="pre">TimePositionalEncoding</span></code></a>(*args, **kwargs)</p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#easy_tpp.model.tf_model.tf_baselayer.TimeShiftedPositionalEncoding" title="easy_tpp.model.tf_model.tf_baselayer.TimeShiftedPositionalEncoding"><code class="xref py py-obj docutils literal notranslate"><span class="pre">TimeShiftedPositionalEncoding</span></code></a>(*args, **kwargs)</p></td>
<td><p></p></td>
</tr>
</tbody>
</table>
<dl class="py function">
<dt class="sig sig-object py" id="easy_tpp.model.tf_model.tf_baselayer.gelu">
<span class="sig-prename descclassname"><span class="pre">easy_tpp.model.tf_model.tf_baselayer.</span></span><span class="sig-name descname"><span class="pre">gelu</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/easy_tpp/model/tf_model/tf_baselayer.html#gelu"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#easy_tpp.model.tf_model.tf_baselayer.gelu" title="Permalink to this definition"></a></dt>
<dd><p>Gaussian Error Linear Unit.
This is a smoother version of the RELU.
Original paper: <a class="reference external" href="https://arxiv.org/abs/1606.08415">https://arxiv.org/abs/1606.08415</a>
:param x: float Tensor to perform activation.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p><cite>x</cite> with the GELU activation applied.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="easy_tpp.model.tf_model.tf_baselayer.append_tensor_alias">
<span class="sig-prename descclassname"><span class="pre">easy_tpp.model.tf_model.tf_baselayer.</span></span><span class="sig-name descname"><span class="pre">append_tensor_alias</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alias</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/easy_tpp/model/tf_model/tf_baselayer.html#append_tensor_alias"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#easy_tpp.model.tf_model.tf_baselayer.append_tensor_alias" title="Permalink to this definition"></a></dt>
<dd><p>Append an alias to the list of aliases of the tensor.
:param tensor: A <cite>Tensor</cite>.
:param alias: String, to add to the list of aliases of the tensor.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>The tensor with a new alias appended to its list of aliases.</p>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="easy_tpp.model.tf_model.tf_baselayer.LayerNormalization">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">easy_tpp.model.tf_model.tf_baselayer.</span></span><span class="sig-name descname"><span class="pre">LayerNormalization</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/easy_tpp/model/tf_model/tf_baselayer.html#LayerNormalization"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#easy_tpp.model.tf_model.tf_baselayer.LayerNormalization" title="Permalink to this definition"></a></dt>
<dd><p>ref: <a class="reference external" href="https://github.com/shenweichen/DeepCTR/blob/master/deepctr/layers/normalization.py">https://github.com/shenweichen/DeepCTR/blob/master/deepctr/layers/normalization.py</a></p>
<dl class="py method">
<dt class="sig sig-object py" id="easy_tpp.model.tf_model.tf_baselayer.LayerNormalization.build">
<span class="sig-name descname"><span class="pre">build</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_shape</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/easy_tpp/model/tf_model/tf_baselayer.html#LayerNormalization.build"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#easy_tpp.model.tf_model.tf_baselayer.LayerNormalization.build" title="Permalink to this definition"></a></dt>
<dd><p>Creates the variables of the layer (optional, for subclass implementers).</p>
<p>This is a method that implementers of subclasses of <cite>Layer</cite> or <cite>Model</cite>
can override if they need a state-creation step in-between
layer instantiation and layer call.</p>
<p>This is typically used to create the weights of <cite>Layer</cite> subclasses.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>input_shape</strong> – Instance of <cite>TensorShape</cite>, or list of instances of
<cite>TensorShape</cite> if the layer expects a list of inputs
(one instance per input).</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="easy_tpp.model.tf_model.tf_baselayer.LayerNormalization.call">
<span class="sig-name descname"><span class="pre">call</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/easy_tpp/model/tf_model/tf_baselayer.html#LayerNormalization.call"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#easy_tpp.model.tf_model.tf_baselayer.LayerNormalization.call" title="Permalink to this definition"></a></dt>
<dd><p>This is where the layer’s logic lives.</p>
<p>Note here that <cite>call()</cite> method in <cite>tf.keras</cite> is little bit different
from <cite>keras</cite> API. In <cite>keras</cite> API, you can pass support masking for
layers as additional arguments. Whereas <cite>tf.keras</cite> has <cite>compute_mask()</cite>
method to support masking.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>inputs</strong> – <p>Input tensor, or dict/list/tuple of input tensors.
The first positional <cite>inputs</cite> argument is subject to special rules:
- <cite>inputs</cite> must be explicitly passed. A layer cannot have zero</p>
<blockquote>
<div><p>arguments, and <cite>inputs</cite> cannot be provided via the default value
of a keyword argument.</p>
</div></blockquote>
<ul>
<li><p>NumPy array or Python scalar values in <cite>inputs</cite> get cast as tensors.</p></li>
<li><p>Keras mask metadata is only collected from <cite>inputs</cite>.</p></li>
<li><p>Layers are built (<cite>build(input_shape)</cite> method)
using shape info from <cite>inputs</cite> only.</p></li>
<li><p><cite>input_spec</cite> compatibility is only checked against <cite>inputs</cite>.</p></li>
<li><p>Mixed precision input casting is only applied to <cite>inputs</cite>.
If a layer has tensor arguments in <cite>*args</cite> or <cite>**kwargs</cite>, their
casting behavior in mixed precision should be handled manually.</p></li>
<li><p>The SavedModel input specification is generated using <cite>inputs</cite> only.</p></li>
<li><p>Integration with various ecosystem packages like TFMOT, TFLite,
TF.js, etc is only supported for <cite>inputs</cite> and not for tensors in
positional and keyword arguments.</p></li>
</ul>
</p></li>
<li><p><strong>*args</strong> – Additional positional arguments. May contain tensors, although
this is not recommended, for the reasons above.</p></li>
<li><p><strong>**kwargs</strong> – <p>Additional keyword arguments. May contain tensors, although
this is not recommended, for the reasons above.
The following optional keyword arguments are reserved:
- <cite>training</cite>: Boolean scalar tensor of Python boolean indicating</p>
<blockquote>
<div><p>whether the <cite>call</cite> is meant for training or inference.</p>
</div></blockquote>
<ul>
<li><p><cite>mask</cite>: Boolean input mask. If the layer’s <cite>call()</cite> method takes a
<cite>mask</cite> argument, its default value will be set to the mask generated
for <cite>inputs</cite> by the previous layer (if <cite>input</cite> did come from a layer
that generated a corresponding mask, i.e. if it came from a Keras
layer with masking support).</p></li>
</ul>
</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A tensor or list/tuple of tensors.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="easy_tpp.model.tf_model.tf_baselayer.MultiHeadAttention">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">easy_tpp.model.tf_model.tf_baselayer.</span></span><span class="sig-name descname"><span class="pre">MultiHeadAttention</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/easy_tpp/model/tf_model/tf_baselayer.html#MultiHeadAttention"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#easy_tpp.model.tf_model.tf_baselayer.MultiHeadAttention" title="Permalink to this definition"></a></dt>
<dd><dl class="py method">
<dt class="sig sig-object py" id="easy_tpp.model.tf_model.tf_baselayer.MultiHeadAttention.build">
<span class="sig-name descname"><span class="pre">build</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_shape</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/easy_tpp/model/tf_model/tf_baselayer.html#MultiHeadAttention.build"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#easy_tpp.model.tf_model.tf_baselayer.MultiHeadAttention.build" title="Permalink to this definition"></a></dt>
<dd><p>Build up weight tensors.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>input_shape</strong> (<em>tensor</em>) – shape of the input.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="easy_tpp.model.tf_model.tf_baselayer.MultiHeadAttention.call">
<span class="sig-name descname"><span class="pre">call</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">attention_input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">attention_mask</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kv</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">training</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/easy_tpp/model/tf_model/tf_baselayer.html#MultiHeadAttention.call"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#easy_tpp.model.tf_model.tf_baselayer.MultiHeadAttention.call" title="Permalink to this definition"></a></dt>
<dd><p>This is where the layer’s logic lives.</p>
<p>Note here that <cite>call()</cite> method in <cite>tf.keras</cite> is little bit different
from <cite>keras</cite> API. In <cite>keras</cite> API, you can pass support masking for
layers as additional arguments. Whereas <cite>tf.keras</cite> has <cite>compute_mask()</cite>
method to support masking.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>inputs</strong> – <p>Input tensor, or dict/list/tuple of input tensors.
The first positional <cite>inputs</cite> argument is subject to special rules:
- <cite>inputs</cite> must be explicitly passed. A layer cannot have zero</p>
<blockquote>
<div><p>arguments, and <cite>inputs</cite> cannot be provided via the default value
of a keyword argument.</p>
</div></blockquote>
<ul>
<li><p>NumPy array or Python scalar values in <cite>inputs</cite> get cast as tensors.</p></li>
<li><p>Keras mask metadata is only collected from <cite>inputs</cite>.</p></li>
<li><p>Layers are built (<cite>build(input_shape)</cite> method)
using shape info from <cite>inputs</cite> only.</p></li>
<li><p><cite>input_spec</cite> compatibility is only checked against <cite>inputs</cite>.</p></li>
<li><p>Mixed precision input casting is only applied to <cite>inputs</cite>.
If a layer has tensor arguments in <cite>*args</cite> or <cite>**kwargs</cite>, their
casting behavior in mixed precision should be handled manually.</p></li>
<li><p>The SavedModel input specification is generated using <cite>inputs</cite> only.</p></li>
<li><p>Integration with various ecosystem packages like TFMOT, TFLite,
TF.js, etc is only supported for <cite>inputs</cite> and not for tensors in
positional and keyword arguments.</p></li>
</ul>
</p></li>
<li><p><strong>*args</strong> – Additional positional arguments. May contain tensors, although
this is not recommended, for the reasons above.</p></li>
<li><p><strong>**kwargs</strong> – <p>Additional keyword arguments. May contain tensors, although
this is not recommended, for the reasons above.
The following optional keyword arguments are reserved:
- <cite>training</cite>: Boolean scalar tensor of Python boolean indicating</p>
<blockquote>
<div><p>whether the <cite>call</cite> is meant for training or inference.</p>
</div></blockquote>
<ul>
<li><p><cite>mask</cite>: Boolean input mask. If the layer’s <cite>call()</cite> method takes a
<cite>mask</cite> argument, its default value will be set to the mask generated
for <cite>inputs</cite> by the previous layer (if <cite>input</cite> did come from a layer
that generated a corresponding mask, i.e. if it came from a Keras
layer with masking support).</p></li>
</ul>
</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A tensor or list/tuple of tensors.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="easy_tpp.model.tf_model.tf_baselayer.DenseDropoutLayernorm">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">easy_tpp.model.tf_model.tf_baselayer.</span></span><span class="sig-name descname"><span class="pre">DenseDropoutLayernorm</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/easy_tpp/model/tf_model/tf_baselayer.html#DenseDropoutLayernorm"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#easy_tpp.model.tf_model.tf_baselayer.DenseDropoutLayernorm" title="Permalink to this definition"></a></dt>
<dd><dl class="py method">
<dt class="sig sig-object py" id="easy_tpp.model.tf_model.tf_baselayer.DenseDropoutLayernorm.call">
<span class="sig-name descname"><span class="pre">call</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">training</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/easy_tpp/model/tf_model/tf_baselayer.html#DenseDropoutLayernorm.call"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#easy_tpp.model.tf_model.tf_baselayer.DenseDropoutLayernorm.call" title="Permalink to this definition"></a></dt>
<dd><p>This is where the layer’s logic lives.</p>
<p>Note here that <cite>call()</cite> method in <cite>tf.keras</cite> is little bit different
from <cite>keras</cite> API. In <cite>keras</cite> API, you can pass support masking for
layers as additional arguments. Whereas <cite>tf.keras</cite> has <cite>compute_mask()</cite>
method to support masking.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>inputs</strong> – <p>Input tensor, or dict/list/tuple of input tensors.
The first positional <cite>inputs</cite> argument is subject to special rules:
- <cite>inputs</cite> must be explicitly passed. A layer cannot have zero</p>
<blockquote>
<div><p>arguments, and <cite>inputs</cite> cannot be provided via the default value
of a keyword argument.</p>
</div></blockquote>
<ul>
<li><p>NumPy array or Python scalar values in <cite>inputs</cite> get cast as tensors.</p></li>
<li><p>Keras mask metadata is only collected from <cite>inputs</cite>.</p></li>
<li><p>Layers are built (<cite>build(input_shape)</cite> method)
using shape info from <cite>inputs</cite> only.</p></li>
<li><p><cite>input_spec</cite> compatibility is only checked against <cite>inputs</cite>.</p></li>
<li><p>Mixed precision input casting is only applied to <cite>inputs</cite>.
If a layer has tensor arguments in <cite>*args</cite> or <cite>**kwargs</cite>, their
casting behavior in mixed precision should be handled manually.</p></li>
<li><p>The SavedModel input specification is generated using <cite>inputs</cite> only.</p></li>
<li><p>Integration with various ecosystem packages like TFMOT, TFLite,
TF.js, etc is only supported for <cite>inputs</cite> and not for tensors in
positional and keyword arguments.</p></li>
</ul>
</p></li>
<li><p><strong>*args</strong> – Additional positional arguments. May contain tensors, although
this is not recommended, for the reasons above.</p></li>
<li><p><strong>**kwargs</strong> – <p>Additional keyword arguments. May contain tensors, although
this is not recommended, for the reasons above.
The following optional keyword arguments are reserved:
- <cite>training</cite>: Boolean scalar tensor of Python boolean indicating</p>
<blockquote>
<div><p>whether the <cite>call</cite> is meant for training or inference.</p>
</div></blockquote>
<ul>
<li><p><cite>mask</cite>: Boolean input mask. If the layer’s <cite>call()</cite> method takes a
<cite>mask</cite> argument, its default value will be set to the mask generated
for <cite>inputs</cite> by the previous layer (if <cite>input</cite> did come from a layer
that generated a corresponding mask, i.e. if it came from a Keras
layer with masking support).</p></li>
</ul>
</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A tensor or list/tuple of tensors.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="easy_tpp.model.tf_model.tf_baselayer.EncoderLayer">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">easy_tpp.model.tf_model.tf_baselayer.</span></span><span class="sig-name descname"><span class="pre">EncoderLayer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/easy_tpp/model/tf_model/tf_baselayer.html#EncoderLayer"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#easy_tpp.model.tf_model.tf_baselayer.EncoderLayer" title="Permalink to this definition"></a></dt>
<dd><dl class="py method">
<dt class="sig sig-object py" id="easy_tpp.model.tf_model.tf_baselayer.EncoderLayer.call">
<span class="sig-name descname"><span class="pre">call</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">training</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/easy_tpp/model/tf_model/tf_baselayer.html#EncoderLayer.call"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#easy_tpp.model.tf_model.tf_baselayer.EncoderLayer.call" title="Permalink to this definition"></a></dt>
<dd><p>This is where the layer’s logic lives.</p>
<p>Note here that <cite>call()</cite> method in <cite>tf.keras</cite> is little bit different
from <cite>keras</cite> API. In <cite>keras</cite> API, you can pass support masking for
layers as additional arguments. Whereas <cite>tf.keras</cite> has <cite>compute_mask()</cite>
method to support masking.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>inputs</strong> – <p>Input tensor, or dict/list/tuple of input tensors.
The first positional <cite>inputs</cite> argument is subject to special rules:
- <cite>inputs</cite> must be explicitly passed. A layer cannot have zero</p>
<blockquote>
<div><p>arguments, and <cite>inputs</cite> cannot be provided via the default value
of a keyword argument.</p>
</div></blockquote>
<ul>
<li><p>NumPy array or Python scalar values in <cite>inputs</cite> get cast as tensors.</p></li>
<li><p>Keras mask metadata is only collected from <cite>inputs</cite>.</p></li>
<li><p>Layers are built (<cite>build(input_shape)</cite> method)
using shape info from <cite>inputs</cite> only.</p></li>
<li><p><cite>input_spec</cite> compatibility is only checked against <cite>inputs</cite>.</p></li>
<li><p>Mixed precision input casting is only applied to <cite>inputs</cite>.
If a layer has tensor arguments in <cite>*args</cite> or <cite>**kwargs</cite>, their
casting behavior in mixed precision should be handled manually.</p></li>
<li><p>The SavedModel input specification is generated using <cite>inputs</cite> only.</p></li>
<li><p>Integration with various ecosystem packages like TFMOT, TFLite,
TF.js, etc is only supported for <cite>inputs</cite> and not for tensors in
positional and keyword arguments.</p></li>
</ul>
</p></li>
<li><p><strong>*args</strong> – Additional positional arguments. May contain tensors, although
this is not recommended, for the reasons above.</p></li>
<li><p><strong>**kwargs</strong> – <p>Additional keyword arguments. May contain tensors, although
this is not recommended, for the reasons above.
The following optional keyword arguments are reserved:
- <cite>training</cite>: Boolean scalar tensor of Python boolean indicating</p>
<blockquote>
<div><p>whether the <cite>call</cite> is meant for training or inference.</p>
</div></blockquote>
<ul>
<li><p><cite>mask</cite>: Boolean input mask. If the layer’s <cite>call()</cite> method takes a
<cite>mask</cite> argument, its default value will be set to the mask generated
for <cite>inputs</cite> by the previous layer (if <cite>input</cite> did come from a layer
that generated a corresponding mask, i.e. if it came from a Keras
layer with masking support).</p></li>
</ul>
</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A tensor or list/tuple of tensors.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="easy_tpp.model.tf_model.tf_baselayer.TimePositionalEncoding">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">easy_tpp.model.tf_model.tf_baselayer.</span></span><span class="sig-name descname"><span class="pre">TimePositionalEncoding</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/easy_tpp/model/tf_model/tf_baselayer.html#TimePositionalEncoding"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#easy_tpp.model.tf_model.tf_baselayer.TimePositionalEncoding" title="Permalink to this definition"></a></dt>
<dd><dl class="py method">
<dt class="sig sig-object py" id="easy_tpp.model.tf_model.tf_baselayer.TimePositionalEncoding.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">hidden_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_len</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">5000</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/easy_tpp/model/tf_model/tf_baselayer.html#TimePositionalEncoding.__init__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#easy_tpp.model.tf_model.tf_baselayer.TimePositionalEncoding.__init__" title="Permalink to this definition"></a></dt>
<dd><p>Standard positional encoder,
source code is from <a class="reference external" href="https://www.tensorflow.org/text/tutorials/transformer#the_encoder_layer">https://www.tensorflow.org/text/tutorials/transformer#the_encoder_layer</a>
:param hidden_size:
:type hidden_size: int
:param max_len:</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="easy_tpp.model.tf_model.tf_baselayer.TimePositionalEncoding.call">
<span class="sig-name descname"><span class="pre">call</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/easy_tpp/model/tf_model/tf_baselayer.html#TimePositionalEncoding.call"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#easy_tpp.model.tf_model.tf_baselayer.TimePositionalEncoding.call" title="Permalink to this definition"></a></dt>
<dd><p>Compute time positional encoding defined in Equation (2) in THP model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>inputs</strong> – (tensor), [batch_size, seq_len]</p></li>
<li><p><strong>**kwargs</strong> – </p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>positional encoding</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="easy_tpp.model.tf_model.tf_baselayer.TimeShiftedPositionalEncoding">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">easy_tpp.model.tf_model.tf_baselayer.</span></span><span class="sig-name descname"><span class="pre">TimeShiftedPositionalEncoding</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/easy_tpp/model/tf_model/tf_baselayer.html#TimeShiftedPositionalEncoding"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#easy_tpp.model.tf_model.tf_baselayer.TimeShiftedPositionalEncoding" title="Permalink to this definition"></a></dt>
<dd><dl class="py method">
<dt class="sig sig-object py" id="easy_tpp.model.tf_model.tf_baselayer.TimeShiftedPositionalEncoding.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">hidden_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_len</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">5000</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/easy_tpp/model/tf_model/tf_baselayer.html#TimeShiftedPositionalEncoding.__init__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#easy_tpp.model.tf_model.tf_baselayer.TimeShiftedPositionalEncoding.__init__" title="Permalink to this definition"></a></dt>
<dd><p>Standard positional encoder,
source code is from <a class="reference external" href="https://www.tensorflow.org/text/tutorials/transformer#the_encoder_layer">https://www.tensorflow.org/text/tutorials/transformer#the_encoder_layer</a>
:param hidden_size:
:type hidden_size: int
:param max_len:</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="easy_tpp.model.tf_model.tf_baselayer.TimeShiftedPositionalEncoding.call">
<span class="sig-name descname"><span class="pre">call</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/easy_tpp/model/tf_model/tf_baselayer.html#TimeShiftedPositionalEncoding.call"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#easy_tpp.model.tf_model.tf_baselayer.TimeShiftedPositionalEncoding.call" title="Permalink to this definition"></a></dt>
<dd><p>Compute time shifted positional encoding defined in Equation (8) in SAHP model</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>inputs</strong> – (time_seqs, time_delta_seqs), [batch_size, seq_len]</p></li>
<li><p><strong>**kwargs</strong> – </p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>positional encoding</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="easy_tpp.model.tf_model.tf_baselayer.DNN">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">easy_tpp.model.tf_model.tf_baselayer.</span></span><span class="sig-name descname"><span class="pre">DNN</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/easy_tpp/model/tf_model/tf_baselayer.html#DNN"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#easy_tpp.model.tf_model.tf_baselayer.DNN" title="Permalink to this definition"></a></dt>
<dd><p>The Multi Layer Percetron
Input shape</p>
<blockquote>
<div><ul class="simple">
<li><p>nD tensor with shape: <code class="docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">...,</span> <span class="pre">input_dim)</span></code>.</p></li>
</ul>
<p>The most common situation would be a 2D input with shape <code class="docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">input_dim)</span></code>.</p>
</div></blockquote>
<dl>
<dt>Output shape</dt><dd><ul class="simple">
<li><p>nD tensor with shape: <code class="docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">...,</span> <span class="pre">hidden_size[-1])</span></code>.</p></li>
</ul>
<p>For instance, for a 2D input with shape <code class="docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">input_dim)</span></code>,
the output would have shape <code class="docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">hidden_size[-1])</span></code>.</p>
</dd>
<dt>Arguments</dt><dd><ul class="simple">
<li><p><strong>hidden_units</strong>:list of positive integer, the layer number and units in each layer.</p></li>
<li><p><strong>activation</strong>: Activation function to use.</p></li>
<li><p><strong>l2_reg</strong>: float between 0 and 1. L2 regularizer strength applied to the kernel weights matrix.</p></li>
<li><p><strong>dropout_rate</strong>: float in [0,1). Fraction of the units to dropout.</p></li>
<li><p><strong>use_bn</strong>: bool. Whether use BatchNormalization before activation or not.</p></li>
<li><p><strong>output_activation</strong>: Activation function to use in the last layer.If <code class="docutils literal notranslate"><span class="pre">None</span></code>,</p></li>
</ul>
<p>it will be same as <code class="docutils literal notranslate"><span class="pre">activation</span></code>.
- <strong>seed</strong>: A Python integer to use as random seed.</p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="easy_tpp.model.tf_model.tf_baselayer.DNN.call">
<span class="sig-name descname"><span class="pre">call</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">training</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/easy_tpp/model/tf_model/tf_baselayer.html#DNN.call"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#easy_tpp.model.tf_model.tf_baselayer.DNN.call" title="Permalink to this definition"></a></dt>
<dd><p>This is where the layer’s logic lives.</p>
<p>Note here that <cite>call()</cite> method in <cite>tf.keras</cite> is little bit different
from <cite>keras</cite> API. In <cite>keras</cite> API, you can pass support masking for
layers as additional arguments. Whereas <cite>tf.keras</cite> has <cite>compute_mask()</cite>
method to support masking.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>inputs</strong> – <p>Input tensor, or dict/list/tuple of input tensors.
The first positional <cite>inputs</cite> argument is subject to special rules:
- <cite>inputs</cite> must be explicitly passed. A layer cannot have zero</p>
<blockquote>
<div><p>arguments, and <cite>inputs</cite> cannot be provided via the default value
of a keyword argument.</p>
</div></blockquote>
<ul>
<li><p>NumPy array or Python scalar values in <cite>inputs</cite> get cast as tensors.</p></li>
<li><p>Keras mask metadata is only collected from <cite>inputs</cite>.</p></li>
<li><p>Layers are built (<cite>build(input_shape)</cite> method)
using shape info from <cite>inputs</cite> only.</p></li>
<li><p><cite>input_spec</cite> compatibility is only checked against <cite>inputs</cite>.</p></li>
<li><p>Mixed precision input casting is only applied to <cite>inputs</cite>.
If a layer has tensor arguments in <cite>*args</cite> or <cite>**kwargs</cite>, their
casting behavior in mixed precision should be handled manually.</p></li>
<li><p>The SavedModel input specification is generated using <cite>inputs</cite> only.</p></li>
<li><p>Integration with various ecosystem packages like TFMOT, TFLite,
TF.js, etc is only supported for <cite>inputs</cite> and not for tensors in
positional and keyword arguments.</p></li>
</ul>
</p></li>
<li><p><strong>*args</strong> – Additional positional arguments. May contain tensors, although
this is not recommended, for the reasons above.</p></li>
<li><p><strong>**kwargs</strong> – <p>Additional keyword arguments. May contain tensors, although
this is not recommended, for the reasons above.
The following optional keyword arguments are reserved:
- <cite>training</cite>: Boolean scalar tensor of Python boolean indicating</p>
<blockquote>
<div><p>whether the <cite>call</cite> is meant for training or inference.</p>
</div></blockquote>
<ul>
<li><p><cite>mask</cite>: Boolean input mask. If the layer’s <cite>call()</cite> method takes a
<cite>mask</cite> argument, its default value will be set to the mask generated
for <cite>inputs</cite> by the previous layer (if <cite>input</cite> did come from a layer
that generated a corresponding mask, i.e. if it came from a Keras
layer with masking support).</p></li>
</ul>
</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A tensor or list/tuple of tensors.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="../ref/models.html" class="btn btn-neutral float-left" title="EasyTPP Models" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="easy_tpp.model.tf_model.tf_basemodel.html" class="btn btn-neutral float-right" title="easy_tpp.model.tf_model.tf_basemodel" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, Machine Intelligence, Alipay.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>